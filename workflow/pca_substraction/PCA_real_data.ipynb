{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304063ba-e882-48b3-b910-4f4813b7acaa",
   "metadata": {},
   "source": [
    "# PCA applied to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830250d-19b9-4257-849b-b454ea24c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046c8fa-f0ba-4cb6-b606-d78cdd0ce9ba",
   "metadata": {},
   "source": [
    "## Let's run the PCA over the 4 x 4 degree cube. We also consider the frequency binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14dbec-890b-44bf-b776-a3c19340c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_eig_decomposition(cube):\n",
    "    \"\"\"\n",
    "    Compute the eigenvectors and eigenvalue decomposition necessary for the \n",
    "    Principal Component Analysis.\n",
    "\n",
    "    :param cube: input data cube. It can be reduced with a mask or not.\n",
    "    \"\"\"\n",
    "    # Make each slide to have mean zero for both the signal and the full simulations\n",
    "    mean_cube_2d =  np.mean(cube, axis=1)\n",
    "    cube -= mean_cube_2d[:, np.newaxis]\n",
    "    # Compute the covariance matrix\n",
    "    cov = np.cov(cube) # (Nfreqs x Nfreqs)\n",
    "    # Do eigendecomposition of covariance matrix\n",
    "    eigvals, eigvecs = np.linalg.eig(cov)\n",
    "    # Sort by eigenvalue\n",
    "    idxs = np.argsort(eigvals)[::-1] # reverse order (biggest eigenvalue first)\n",
    "    eigvals = eigvals[idxs]\n",
    "    eigvecs = eigvecs[:,idxs]\n",
    "    return cube, mean_cube_2d, eigvals, eigvecs\n",
    "\n",
    "def proyect_PCA(nmodes, eigvecs, cube2d, mean_cube2d):\n",
    "    \"\"\"\n",
    "    Function that removes the foregrounds using the PCA decomposition. It removes the first nmodes \n",
    "    obtained from the PCA decomposition.\n",
    "\n",
    "    :param nmodes: number of modes to subtract\n",
    "    :param eigvecs: eigenvectors from the PCA decomposition.\n",
    "    :param cube2d: data cube with dimensions nsamples x Npix**2.\n",
    "    :param mean_cube2d: mean of each slice of cube2d.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct foreground filter operator by keeping only nmodes eigenmodes\n",
    "    U_fg = eigvecs[:,:nmodes] # (Nfreqs, Nmodes)\n",
    "    \n",
    "    # Calculate foreground amplitudes for each line of sight\n",
    "    fg_amps = np.dot(U_fg.T, cube2d) # (Nmodes, Npix**2)\n",
    "    \n",
    "    # Construct FG field and subtract from input data\n",
    "    fg_field = np.dot(U_fg, fg_amps) + mean_cube2d[:, np.newaxis] # Operator times amplitudes + mean\n",
    "    shape1 = (nmodes, int(np.sqrt(cube2d.shape[1])), int(np.sqrt(cube2d.shape[1])))\n",
    "    shape2 = (int(cube2d.shape[0]), int(np.sqrt(cube2d.shape[1])), int(np.sqrt(cube2d.shape[1])))\n",
    "    return fg_amps.reshape(shape1), fg_field.reshape(shape2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fc282-d93a-40df-a4a8-a5f64c1e60c0",
   "metadata": {},
   "source": [
    "## Load the data cube\n",
    "\n",
    "The full frequency range of 90 MHz is divided into 15 MHz intervals, and for each a power spectrum is computed. To limit noise, only the central 4x4 degrees out of the full 8x8 degrees FoV are meant to be used for the power spectrum computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd26f4-c033-4d00-a3a9-6c4a7d32925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_msn = '/mnt/sdc3a.MS.0/sdc3dataset/Image/ZW3.msn_image.fits'\n",
    "fname_msw = '/mnt/sdc3a.MS.0/sdc3dataset/Image/ZW3.msw_image.fits'\n",
    "data_shape = fits.open(fname_msw)[0].data.shape # shape of the data cube\n",
    "header = fits.open(fname_msw)[0].header\n",
    "angle = 4 # in degrees\n",
    "resolution = 16 # in arcsecs\n",
    "Npix_ps = angle*3600/resolution # number of pixels in the output image\n",
    "Npix_data = data_shape[1] # number of pixels in the x-axis in the data\n",
    "ind1 = int(Npix_data/2 - Npix_ps/2)\n",
    "ind2 = int(Npix_data/2 + Npix_ps/2)\n",
    "freq = np.arange(106, 196 + 0.1, 0.1)\n",
    "freq_int = np.arange(0, 900 + 0.1, 150, dtype=int)\n",
    "freq_int[len(freq_int)-1] += 1\n",
    "data_ps = []\n",
    "for i in tqdm(range(len(freq_int)-1)):\n",
    "    data_ps.append(fits.open(fname_msw)[0].data[freq_int[i]:freq_int[i+1], ind1:ind2, ind1:ind2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da329483-ff0f-4ae1-bb69-54853b93c9c4",
   "metadata": {},
   "source": [
    "## Load the point sources mask\n",
    "\n",
    "The file named \"predictions.fit\" contains not binary values of 0 or 1, but the probabilities produced by the model's output. These probabilistic masks are processed based on a predetermined threshold. For example, if a threshold of 0.4 is set, pixels with probabilities above this value will be marked as '1', and the rest as '0'.\n",
    "\n",
    "We generate one foreground mask per frequency bin. This is done by calculating the maximum of the pixels in the slices in the frequency interval of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49278d62-b0c4-4df6-a54e-4cd2e832c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_mask = '/mnt/scratch/ps_masks/predictions.fit'\n",
    "mask = fits.open(fname_mask)[0].data[:, ind1:ind2, ind1:ind2, 0]\n",
    "threshold = 0.3 \n",
    "mask_thresh = np.zeros_like(mask, dtype=int)\n",
    "mask_thresh[mask > threshold] = 1\n",
    "mask_binned = []\n",
    "mask_neg = []\n",
    "for i in tqdm(range(len(freq_int)-1)):\n",
    "    # We create a common mask for all the slices.\n",
    "    mask_binned.append(np.maximum.reduce(mask_thresh[freq_int[i]:freq_int[i+1], :, :], axis=0))\n",
    "    mask_neg.append(np.zeros_like(mask_binned[i]))\n",
    "    mask_neg[i][mask_binned[i] == 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3b5fb-afc0-4ac5-ae52-8245cf70dba2",
   "metadata": {},
   "source": [
    "## Load the PFS and reduce its shape in order to match the dimension of the data and point sources mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecca382-0399-4f40-bfb6-4f905349f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_station_beam = '/mnt/sdc3a.MS.0/sdc3dataset/Image/station_beam.fits'\n",
    "data_station_beam_shape = fits.open(fname_station_beam)[0].data.shape\n",
    "Npix_beam = data_station_beam_shape[1]\n",
    "index1_beam = int(Npix_beam/2 - Npix_ps/2)\n",
    "index2_beam = int(Npix_beam/2 + Npix_ps/2)\n",
    "beam = []\n",
    "for i in tqdm(range(len(freq_int)-1)):\n",
    "    beam.append(fits.open(fname_station_beam)[0].data[freq_int[i]:freq_int[i+1], index1_beam:index2_beam, index1_beam:index2_beam])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f35ae5-e1e3-464f-8a4d-46aef9d9b61b",
   "metadata": {},
   "source": [
    "## We will run PCA following this steps:\n",
    "\n",
    "    1) Run PCA on ncomponents = 4 (from which simulations have shown to be ideal), to get the projection matrix.\n",
    "\n",
    "$$d\\rightarrow P = WW^T.$$\n",
    "\n",
    "    2) Project the PCA solution back to the image space from the components space.\n",
    "\n",
    "$$d^{PCA} = P\\cdot d.$$\n",
    "\n",
    "    3) Obtain the clean solution from substraction of the PCA result to the data.\n",
    "$$d^{clean} = d-d^{PCA}.$$\n",
    "\n",
    "Products that I generated from the PCA cleaned solution.\n",
    "\n",
    "    1) Cleaned PCA solution.\n",
    "\n",
    "    2) Cleaned and smoothed PCA solution.\n",
    "\n",
    "    3) Cleaned, masked and smoothed solution.\n",
    "\n",
    "I do not do PSF deconvolution in any of the data products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f40e9-79ac-4bf4-a496-59fc82ebab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfreq_bins = (len(freq_int)-1)\n",
    "ncomp = 4\n",
    "cube_2d = [None] * nfreq_bins\n",
    "mean_cube_2d = [None] * nfreq_bins\n",
    "eigvals = [None] * nfreq_bins\n",
    "eigvecs = [None] * nfreq_bins\n",
    "fg_amps = [None] * nfreq_bins\n",
    "fg_field = [None] * nfreq_bins\n",
    "clean_field = [None] * nfreq_bins\n",
    "clean_field_masked = [None] * nfreq_bins\n",
    "for i in tqdm(range(len(freq_int)-1)):\n",
    "    nsamples, nx, ny = data_ps[i].shape\n",
    "    cube_2d[i] = data_ps[i].reshape((nsamples,nx*ny))\n",
    "    cube_2d[i], mean_cube_2d[i], eigvals[i], eigvecs[i] = PCA_eig_decomposition(np.array(cube_2d[i]))\n",
    "    fg_amps[i], fg_field[i] = proyect_PCA(ncomp, eigvecs[i], cube_2d[i], mean_cube_2d[i])\n",
    "    clean_field[i] = data_ps[i] - fg_field[i]\n",
    "    clean_field_masked[i] = clean_field[i] * mask_neg[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f94f47-4dd1-4b0b-b5a6-6dc7ea9c72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "for i in tqdm(range(len(freq_int)-1)):\n",
    "    plt.plot(np.cumsum(eigvals[i])/np.sum(eigvals[i]), label=f'Bin = {i}')\n",
    "plt.xlabel('N components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.semilogx()\n",
    "plt.ylim([0.99980, 1.00001])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6b7cc-5b48-48dd-bcac-0e00d569f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 50\n",
    "from pylab import figure, cm\n",
    "from matplotlib.colors import LogNorm\n",
    "import scipy.ndimage as ndimage\n",
    "# Define the number of rows and columns\n",
    "nrows = 6\n",
    "ncols = 4\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,25))\n",
    "# Iterate over rows\n",
    "for i in range(nrows):\n",
    "    im0 = axes[i, 0].imshow(np.log10(np.abs(data_ps[i][slice, :, :]))*mask_neg[i])\n",
    "    plt.colorbar(im0, ax=axes[i, 0], orientation='vertical')\n",
    "    im1 = axes[i, 1].imshow(np.log10(np.abs(fg_field[i][slice, :, :]))*mask_neg[i])\n",
    "    plt.colorbar(im1, ax=axes[i, 1], orientation='vertical')\n",
    "    im2 = axes[i, 2].imshow(clean_field[i][slice, :, :])\n",
    "    plt.colorbar(im2, ax=axes[i, 2], orientation='vertical')\n",
    "    im3 = axes[i, 3].imshow(ndimage.gaussian_filter(clean_field[i][slice, :, :], sigma=5, order=0))\n",
    "    plt.colorbar(im3, ax=axes[i, 3], orientation='vertical')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ac8c2-a2f3-4cdc-a6b1-26ea21b762b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_field_smooth = [None] * nfreq_bins\n",
    "clean_field_masked_smooth = [None] * nfreq_bins\n",
    "for bin in tqdm(range(len(freq_int)-1)):\n",
    "    clean_field_smooth[bin] = np.zeros_like(clean_field[bin])\n",
    "    clean_field_masked_smooth[bin] = np.zeros_like(clean_field[bin])\n",
    "    for slice in range(clean_field[bin].shape[0]):\n",
    "        clean_field_smooth[bin][slice, :, :] = ndimage.gaussian_filter(clean_field[bin][slice, :, :], sigma=5, order=0)\n",
    "        clean_field_masked_smooth[bin][slice, :, :] = ndimage.gaussian_filter(clean_field_masked[bin][slice, :, :], sigma=5, order=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6052ee6-7359-427f-b5ea-3f0e7c58aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 50\n",
    "from pylab import figure, cm\n",
    "from matplotlib.colors import LogNorm\n",
    "import scipy.ndimage as ndimage\n",
    "# Define the number of rows and columns\n",
    "nrows = 6\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,25))\n",
    "# Iterate over rows\n",
    "for i in range(nrows):\n",
    "    im0 = axes[i, 0].imshow(clean_field[bin][slice, :, :])\n",
    "    plt.colorbar(im0, ax=axes[i, 0], orientation='vertical')\n",
    "    im1 = axes[i, 1].imshow(clean_field_smooth[i][slice, :, :])\n",
    "    plt.colorbar(im1, ax=axes[i, 1], orientation='vertical')\n",
    "    im2 = axes[i, 2].imshow(clean_field_masked_smooth[i][slice, :, :])\n",
    "    plt.colorbar(im2, ax=axes[i, 2], orientation='vertical')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee416713-7e4d-44f0-9f43-5cdb8fede954",
   "metadata": {},
   "source": [
    "## We save our data products in several fits files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44dd84d-8670-46a3-8d2a-1e47e4a252c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('data_products', exist_ok=True)\n",
    "\n",
    "for bin in tqdm(range(len(freq_int)-1)):\n",
    "    fname = f'data_products/PCA_cleaned_{freq_int[bin]}_{freq_int[bin+1]}.fits'\n",
    "    fname_smooth = f'data_products/PCA_cleaned_smooth_{freq_int[bin]}_{freq_int[bin+1]}.fits'\n",
    "    fname_smooth_masked = f'data_products/PCA_cleaned_smooth_mask_{freq_int[bin]}_{freq_int[bin+1]}.fits'\n",
    "\n",
    "    # Create a FITS header by modifying the one from the data fits file\n",
    "    header['NAXIS1'] = clean_field[bin].shape[2]\n",
    "    header['NAXIS2'] = clean_field[bin].shape[1]\n",
    "    header['NAXIS3'] = clean_field[bin].shape[0]\n",
    "    header['CRVAL3'] = freq[freq_int[bin]]*1e6 # in Hz\n",
    "    # Create a FITS HDU (Header/Data Unit) containing the matrix\n",
    "    hdu_clean = fits.PrimaryHDU(data=clean_field[bin], header=header)\n",
    "    hdu_clean_smooth = fits.PrimaryHDU(data=clean_field_smooth[bin], header=header)\n",
    "    hdu_clean_smooth_mask = fits.PrimaryHDU(data=clean_field_masked_smooth[bin], header=header)\n",
    "    # Create a HDU list and add the primary HDU to it\n",
    "    hdul_clean = fits.HDUList([hdu_clean])\n",
    "    hdul_clean_smooth = fits.HDUList([hdu_clean_smooth])\n",
    "    hdul_clean_smooth_mask = fits.HDUList([hdu_clean_smooth])\n",
    "    \n",
    "    # Write the HDU list to the FITS file\n",
    "    hdul_clean.writeto(fname, overwrite=True)\n",
    "    hdul_clean_smooth.writeto(fname_smooth, overwrite=True)\n",
    "    hdul_clean_smooth_mask.writeto(fname_smooth_masked, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
